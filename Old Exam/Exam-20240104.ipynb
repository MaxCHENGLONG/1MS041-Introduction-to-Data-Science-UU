{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Exam 4th of January 2024, 8.00-13.00 for the course 1MS041 (Introduction to Data Science / Introduktion till dataanalys)\n",
    "\n",
    "## Instructions:\n",
    "1. Complete the problems by following instructions.\n",
    "2. When done, submit this file with your solutions saved, following the instruction sheet.\n",
    "\n",
    "This exam has 3 problems for a total of 40 points, to pass you need\n",
    "20 points. The bonus will be added to the score of the exam and rounded afterwards.\n",
    "\n",
    "## Some general hints and information:\n",
    "* Try to answer all questions even if you are uncertain.\n",
    "* Comment your code, so that if you get the wrong answer I can understand how you thought\n",
    "this can give you some points even though the code does not run.\n",
    "* Follow the instruction sheet rigorously.\n",
    "* This exam is partially autograded, but your code and your free text answers are manually graded anonymously.\n",
    "* If there are any questions, please ask the exam guards, they will escalate it to me if necessary.\n",
    "\n",
    "## Tips for free text answers\n",
    "* Be VERY clear with your reasoning, there should be zero ambiguity in what you are referring to.\n",
    "* If you want to include math, you can write LaTeX in the Markdown cells, for instance `$f(x)=x^2$` will be rendered as $f(x)=x^2$ and `$$f(x) = x^2$$` will become an equation line, as follows\n",
    "$$f(x) = x^2$$\n",
    "Another example is `$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$` which renders as\n",
    "$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$\n",
    "\n",
    "## Finally some rules:\n",
    "* You may not communicate with others during the exam, for example:\n",
    "    * You cannot ask for help in Stack-Overflow or other such help forums during the Exam.\n",
    "    * You may not communicate with AI's, for instance ChatGPT.\n",
    "    * Your on-line and off-line activity is being monitored according to the examination rules.\n",
    "\n",
    "## Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Insert your anonymous exam ID as a string in the variable below\n",
    "examID=\"0019-BYZ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "\n",
    "1. [4p] Fill in the remaining part of the function `problem1_inversion` in order to produce samples from the below distribution using rejection sampling:\n",
    "\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        \\frac{e^{x^2}-1}{e-1}, & 0 < x < 1 \\\\\n",
    "        1, & x \\geq 1\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "2. [2p] Produce 100000 samples (**use fewer if it times-out and you cannot find a solution**) and put the answer in `problem1_samples` from the above distribution and plot the histogram together with the true density. *(There is a timeout decorator on this function and if it takes more than 10 seconds to generate 100000 samples it will timeout and it will count as if you failed to generate.)*\n",
    "3. [2p] Use the above 100000 samples (`problem1_samples`) to approximately compute the integral\n",
    "\n",
    "$$\n",
    "    \\int_0^{1} \\sin(x) \\frac{2e^{x^2} x}{e-1} dx\n",
    "$$\n",
    "and store the result in `problem1_integral`.\n",
    "\n",
    "4. [2p] Use Hoeffdings inequality to produce a 95\\% confidence interval of the integral above and store the result as a tuple in the variable `problem1_interval`\n",
    "\n",
    "5. [4p] Fill in the remaining part of the function `problem1_inversion_2` in order to produce samples from the below distribution using rejection sampling:\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        20xe^{20-1/x}, & 0 < x < \\frac{1}{20} \\\\\n",
    "        1, & x \\geq \\frac{1}{20}\n",
    "    \\end{cases}\n",
    "$$\n",
    "Hint: this is tricky because if you choose the wrong sampling distribution you reject at least 9 times out of 10. You will get points based on how long your code takes to create a certain number of samples, if you choose the correct sampling distribution you can easily create 100000 samples within 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Part 1\n",
    "\n",
    "from Utils import timeout\n",
    "import random\n",
    "#@timeout\n",
    "def problem1_inversion(n_samples=1):\n",
    "    # Distribution from part 1\n",
    "    # write the code in this function to produce samples from the distribution in the assignment\n",
    "    # Make sure you choose a good sampling distribution to avoid unnecessary rejections\n",
    "    answer = [ ]\n",
    "    for t in range(n_samples):\n",
    "        y = random.random()\n",
    "        if y <= 0:\n",
    "            x = 0\n",
    "            answer.append(x)\n",
    "        elif y < 1 and y > 0:\n",
    "            x = (np.exp(y ** 2) - 1 )/(np.exp(1) - 1)\n",
    "            answer.append(x)\n",
    "        else:\n",
    "            x = 1\n",
    "            answer.apppend(x)\n",
    "    # Return a numpy array of length n_samples\n",
    "    return np.asarray(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ang-tenta\\AppData\\Local\\Temp\\ipykernel_27220\\1638456695.py:9: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(problem1_samples, hist=True, kde=True,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 2\n",
    "import numpy as np\n",
    "problem1_samples = problem1_inversion(100000)\n",
    "#print(problem1_samples)\n",
    "#problem1_samplesdist =np.array(problem1_samples)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.hist(problem1_samples)\n",
    "sns.distplot(problem1_samples, hist=True, kde=True, \n",
    "             bins=int(100000/1000), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "plt.show()\n",
    "#problem1_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27451961599495833\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "import math\n",
    "import numpy as np\n",
    "def MC(samples,n_samples): \n",
    "    integral_value = 0\n",
    "    for i in range(n_samples):\n",
    "        x = samples[i]\n",
    "        integral_value = integral_value + (1-0)*(math.sin(x) * 2 * np.exp(x**2)*x )/(np.exp(1) -1)\n",
    "    integral_value = integral_value/n_samples\n",
    "    return integral_value\n",
    "problem1_integral = MC(problem1_samples,len(problem1_samples))\n",
    "print(problem1_integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2654390296920522, 0.27402841785898696)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 4\n",
    "from numpy import *\n",
    "def hoeffding_interval(sample_mean, n, a, b, confidence_level=0.95):\n",
    "    epsilon = np.sqrt(np.log(2 / (1 - confidence_level)) / (2 * n))\n",
    "    return max(a, sample_mean - epsilon * (b - a)), min(b, sample_mean + epsilon * (b - a))\n",
    "problem1_samples_avg = np.mean(problem1_samples)\n",
    "n = len(problem1_samples)\n",
    "a = 0.0 \n",
    "b = 1.0\n",
    "problem1_interval = hoeffding_interval(problem1_samples_avg,n,a,b)\n",
    "problem1_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [],
   "source": [
    "# Part 5\n",
    "import random\n",
    "import numpy as np\n",
    "def problem1_inversion_2(n_samples=1):\n",
    "    # Distribution from part 2\n",
    "    # write the code in this function to produce samples from the distribution in the assignment\n",
    "    # Make sure you choose a good sampling distribution to avoid unnecessary rejections\n",
    "    samples = [ ]\n",
    "    for t in range(n_samples):\n",
    "        y = random.random()\n",
    "        if y <= 0:\n",
    "            x = 0\n",
    "            samples.append(x)\n",
    "        elif y > 0 and y < 1/20:\n",
    "            x = 20 * x * np.exp(20- 1/x )\n",
    "            samples.append(x)\n",
    "        else:\n",
    "            x = 1\n",
    "            samples.append(x)\n",
    "    # Return a numpy array of length n_samples\n",
    "    return np.asarray(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "#### Local Test for Exam vB, PROBLEM 1\n",
    "Evaluate cell below to make sure your answer is valid.                             You **should not** modify anything in the cell below when evaluating it to do a local test of                             your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good, your problem1_inversion returns a numpy array\n",
      "Good, your problem1_samples is a numpy array\n",
      "Good, your problem1_integral is a float\n",
      "Good, your problem1_interval is a tuple or list of length 2\n",
      "Good, your problem1_inversion_2 returns a numpy array\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This cell is just to check that you got the correct formats of your answer\n",
    "import numpy as np\n",
    "try:\n",
    "    assert(isinstance(problem1_inversion(10), np.ndarray)) \n",
    "except:\n",
    "    print(\"Try again. You should return a numpy array from problem1_inversion\")\n",
    "else:\n",
    "    print(\"Good, your problem1_inversion returns a numpy array\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_samples, np.ndarray)) \n",
    "except:\n",
    "    print(\"Try again. your problem1_samples is not a numpy array\")\n",
    "else:\n",
    "    print(\"Good, your problem1_samples is a numpy array\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_integral, float)) \n",
    "except:\n",
    "    print(\"Try again. your problem1_integral is not a float\")\n",
    "else:\n",
    "    print(\"Good, your problem1_integral is a float\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_interval, list) or isinstance(problem1_interval, tuple)) , \"problem1_interval not a tuple or list\"\n",
    "    assert(len(problem1_interval) == 2) , \"problem1_interval does not have length 2, it should have a lower bound and an upper bound\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"Good, your problem1_interval is a tuple or list of length 2\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_inversion_2(10), np.ndarray)) \n",
    "except:\n",
    "    print(\"Try again. You should return a numpy array from problem1_inversion_2\")\n",
    "else:\n",
    "    print(\"Good, your problem1_inversion_2 returns a numpy array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "source": [
    "\n",
    "Let us build a proportional model ($\\mathbb{P}(Y=1 \\mid X) = G(\\beta_0+\\beta \\cdot X)$ where $G$ is the logistic function) for the spam vs not spam data. Here we assume that the features are presence vs not presence of a word, let $X_1,X_2,X_3$ denote the presence (1) or absence (0) of the words $(\"free\", \"prize\", \"win\")$.\n",
    "\n",
    "1. [2p] Load the file `data/spam.csv` and create two numpy arrays, `problem2_X` which has shape (n_emails,3) where each feature in `problem2_X` corresponds to $X_1,X_2,X_3$ from above, `problem2_Y` which has shape **(n_emails,)** and consists of a $1$ if the email is spam and $0$ if it is not. Split this data into a train-calibration-test sets where we have the split $40\\%$, $20\\%$, $40\\%$, put this data in the designated variables in the code cell.\n",
    "\n",
    "2. [4p] Follow the calculation from the lecture notes where we derive the logistic regression and implement the final loss function inside the class `ProportionalSpam`. You can use the `Test` cell to check that it gives the correct value for a test-point.\n",
    "\n",
    "3. [4p] Train the model `problem2_ps` on the training data. The goal is to calibrate the probabilities output from the model. Start by creating a new variable `problem2_X_pred` (shape `(n_samples,1)`) which consists of the predictions of `problem2_ps` on the calibration dataset. Then train a calibration model using `sklearn.tree.DecisionTreeRegressor`, store this trained model in `problem2_calibrator`.\n",
    "\n",
    "4. [3p] Use the trained model `problem2_ps` and the calibrator `problem2_calibrator` to make final predictions on the testing data, store the prediction in `problem2_final_predictions`. Compute the $0-1$ test-loss and store it in `problem2_01_loss` and provide a $99\\%$ confidence interval of it, store this in the variable `problem2_interval`, this should again be a tuple as in **problem1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2228, 3) (1115, 3) (2229, 3) (2228,) (1115,) (2229,)\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "def load_sms():\n",
    "    import csv\n",
    "    lines = []\n",
    "    hamspam = {'ham': 0, 'spam': 1}\n",
    "    with open('data/spam.csv', mode='r',encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        lines = [(line[1],hamspam[line[0]]) for line in reader]\n",
    "        \n",
    "    return lines\n",
    "spam_no_spam = load_sms()\n",
    "#print(spam_no_spam)\n",
    "x1 = [ ]\n",
    "x2 = [ ]\n",
    "x3 = [ ]\n",
    "def getx(dataset,keyword):\n",
    "    x = []\n",
    "\n",
    "    for sms, is_spam in dataset:\n",
    "        # Splitting the SMS into words\n",
    "        words = sms.lower().split()\n",
    "        # Check if 'free' or 'prize' are present as complete words\n",
    "        has_keywords = keyword in words\n",
    "\n",
    "\n",
    "        if has_keywords:\n",
    "            x.append('1')\n",
    "        else:\n",
    "            x.append('0')\n",
    "    return x\n",
    "x1 = getx(spam_no_spam,\"free\")\n",
    "x2 = getx(spam_no_spam,\"prize\")\n",
    "x3 = getx(spam_no_spam,\"win\")\n",
    "problem2_X = np.array([x1,x2,x3])\n",
    "problem2_X = problem2_X.transpose()\n",
    "def getY(dataset):\n",
    "    y = [ ]\n",
    "    for sms, is_spam in dataset:\n",
    "        if is_spam:\n",
    "            y.append('1')\n",
    "        else:\n",
    "            y.append('0')\n",
    "    return y\n",
    "Y = getY(spam_no_spam)\n",
    "Y = np.array(Y) \n",
    "problem2_Y = Y\n",
    "#print(problem2_X.shape)\n",
    "#print(problem2_Y.shape)\n",
    "def train_test_validation(X,Y,test_size=0.2,validation_size=0.2,random_state=None,shuffle=True):\n",
    "    \"\"\"\n",
    "    Performs a train test validation split of the data [train_data,test_data,validation_data]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : The input X, shape (n_samples,n_features)\n",
    "    Y : The input labells, shape (n_samples)\n",
    "    test_size : the proportion of data that should be test data\n",
    "    validation_size : the proportion of data that should be validation data\n",
    "    random_state : the random state variable passed through to sklearns train_test_split\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    X_train, X_test, X_valid, Y_train, Y_test, Y_valid\n",
    "\n",
    "    Examples:\n",
    "    ----------\n",
    "    >>> X_train, X_test, X_valid, Y_train, Y_test, Y_valid = train_test_validation(X,Y,test_size=0.25,validation_size=0.25)\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train,X_tt,Y_train,Y_tt = train_test_split(X,Y,\n",
    "                                                 test_size=test_size+validation_size,\n",
    "                                                 random_state=random_state,\n",
    "                                                 shuffle=shuffle)\n",
    "    X_test,X_valid,Y_test,Y_valid = train_test_split(X_tt,Y_tt,\n",
    "                                                     test_size=(validation_size)/(test_size + validation_size),\n",
    "                                                     random_state=random_state,\n",
    "                                                     shuffle=shuffle)\n",
    "\n",
    "    return X_train, X_test, X_valid, Y_train, Y_test, Y_valid\n",
    "\n",
    "#problem2_X_train = XXX\n",
    "#problem2_X_calib = XXX\n",
    "#problem2_X_test = XXX\n",
    "\n",
    "#problem2_Y_train = XXX\n",
    "#problem2_Y_calib = XXX\n",
    "#problem2_Y_test = XXX\n",
    "problem2_X_train,problem2_X_test,problem2_X_calib,problem2_Y_train,problem2_Y_test,problem2_Y_calib = train_test_validation(problem2_X,problem2_Y,test_size = 0.4,validation_size = 0.2)\n",
    "print(problem2_X_train.shape,problem2_X_calib.shape,problem2_X_test.shape,problem2_Y_train.shape,problem2_Y_calib.shape,problem2_Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "class ProportionalSpam(object):\n",
    "    def __init__(self):\n",
    "        self.coeffs = None\n",
    "        self.result = None\n",
    "    \n",
    "    # define the objective/cost/loss function we want to minimise\n",
    "    def loss(self,X,Y,coeffs):\n",
    "        #cost = np.transpose(-y)@np.log(h) - np.transpose(1-y)@np.log(1-h) + (l/2)*np.transpose(t[1:])@t[1:]\n",
    "        #cost = (1/m)*cost\n",
    "        #return cost\n",
    "        y_hat = self.predict(X)\n",
    "        lossvalue = -np.mean(Y*np.log(y_hat) + (1- Y )*np.log(1-y_hat))\n",
    "        return lossvalue\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        import numpy as np\n",
    "        from scipy import optimize\n",
    "\n",
    "        #Use the f above together with an optimization method from scipy\n",
    "        #to find the coefficients of the model\n",
    "        opt_loss = lambda coeffs: self.loss(X,Y,coeffs)\n",
    "        initial_arguments = np.zeros(shape=X.shape[1]+1)\n",
    "        self.result = optimize.minimize(opt_loss, initial_arguments,method='cg')\n",
    "        self.coeffs = self.result.x\n",
    "    \n",
    "    def predict(self,X):\n",
    "        #Use the trained model to predict Y\n",
    "        if (self.coeffs is not None):\n",
    "            G = lambda x: np.exp(x)/(1+np.exp(x))\n",
    "            return np.round(10*G(np.dot(X,self.coeffs[1:])+self.coeffs[0]))/10 # This rounding is to help you with the calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type NoneType which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Part 3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m problem2_ps \u001b[38;5;241m=\u001b[39m ProportionalSpam()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mproblem2_ps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem2_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproblem2_Y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m problem2_X_pred \u001b[38;5;241m=\u001b[39m problem2_ps\u001b[38;5;241m.\u001b[39mpredict(problem2_X_calib)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#train DecisonTreeRegressor\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m, in \u001b[0;36mProportionalSpam.fit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     23\u001b[0m opt_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m coeffs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(X,Y,coeffs)\n\u001b[0;32m     24\u001b[0m initial_arguments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_arguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:689\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    687\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_powell(fun, x0, args, callback, bounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 689\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    691\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1686\u001b[0m, in \u001b[0;36m_minimize_cg\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m-> 1686\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1689\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m   1690\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    328\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mProportionalSpam.fit.<locals>.<lambda>\u001b[1;34m(coeffs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#Use the f above together with an optimization method from scipy\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#to find the coefficients of the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m opt_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m coeffs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcoeffs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m initial_arguments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(opt_loss, initial_arguments,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mProportionalSpam.loss\u001b[1;34m(self, X, Y, coeffs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Y,coeffs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#cost = np.transpose(-y)@np.log(h) - np.transpose(1-y)@np.log(1-h) + (l/2)*np.transpose(t[1:])@t[1:]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#cost = (1/m)*cost\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#return cost\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m---> 14\u001b[0m     lossvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(Y\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m Y )\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_hat))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lossvalue\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type NoneType which has no callable log method"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "\n",
    "problem2_ps = ProportionalSpam()\n",
    "problem2_ps.fit(problem2_X_train,problem2_Y_train)\n",
    "problem2_X_pred = problem2_ps.predict(problem2_X_calib)\n",
    "\n",
    "\n",
    "#train DecisonTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "problem2_calibrator = DecisionTreeRegressor()\n",
    "problem2_calibrator.fit(problem2_X_train,problem2_Y_train)\n",
    "DecisionTreeRegressorproblem2_X_pred = problem2_calibrator.predict(problem2_X_calib)\n",
    "#print(DecisionTreeRegressorproblem2_X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types (dtype('float64'), dtype('<U1')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m problem2_final_predictions \u001b[38;5;241m=\u001b[39m problem2_calibrator\u001b[38;5;241m.\u001b[39mpredict(problem2_X_test)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# In order to compute this loss we first need to convert the predicted probabilities to a decision\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# recall the Bayes classifier?\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m problem2_01_loss \u001b[38;5;241m=\u001b[39m (\u001b[43mproblem2_final_predictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproblem2_Y_test\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Recall the interval is given as a tuple (a,b) or a list [a,b]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhoeffding_interval\u001b[39m(sample_mean, n, a, b, confidence_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m):\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types (dtype('float64'), dtype('<U1')) -> None"
     ]
    }
   ],
   "source": [
    "# Part 4\n",
    "import numpy as np\n",
    "# These are the predicted probabilities\n",
    "problem2_final_predictions = problem2_calibrator.predict(problem2_X_test)\n",
    "\n",
    "\n",
    "# In order to compute this loss we first need to convert the predicted probabilities to a decision\n",
    "# recall the Bayes classifier?\n",
    "problem2_01_loss = (problem2_final_predictions - problem2_Y_test).sum()\n",
    "\n",
    "# Recall the interval is given as a tuple (a,b) or a list [a,b]\n",
    "def hoeffding_interval(sample_mean, n, a, b, confidence_level=0.95):\n",
    "    epsilon = np.sqrt(np.log(2 / (1 - confidence_level)) / (2 * n))\n",
    "    return max(a, sample_mean - epsilon * (b - a)), min(b, sample_mean + epsilon * (b - a))\n",
    "sa_avg = np.mean(problem2_final_predictions)\n",
    "n = len(problem2_final_predictions)\n",
    "a = 0.0\n",
    "b = 1.0 \n",
    "problem2_interval = hoeffding_interval(sa_avg,n,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "source": [
    "---\n",
    "#### Local Test for Exam vB, PROBLEM 2\n",
    "Evaluate cell below to make sure your answer is valid.                             You **should not** modify anything in the cell below when evaluating it to do a local test of                             your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss was not correct on a test point\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    test_instance = ProportionalSpam()\n",
    "    test_loss = test_instance.loss(np.array([[1,0,1],[0,1,1]]),np.array([1,0]),np.array([1.2,0.4,0.3,0.9]))\n",
    "    assert (np.abs(test_loss-1.2828629432232497) < 1e-6)\n",
    "    print(\"Your loss was correct for a test point\")\n",
    "except:\n",
    "    print(\"Your loss was not correct on a test point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "source": [
    "\n",
    "Consider the following four Markov chains, answer each question for all chains:\n",
    "\n",
    "<img width=\"400px\" src=\"pictures/MarkovA.png\">Markov chain A</img>\n",
    "<img width=\"400px\" src=\"pictures/MarkovB.png\">Markov chain B</img>\n",
    "<img width=\"400px\" src=\"pictures/MarkovC.png\">Markov chain C</img>\n",
    "<img width=\"400px\" src=\"pictures/MarkovD.png\">Markov chain D</img>\n",
    "\n",
    "1. [2p] What is the transition matrix?\n",
    "2. [2p] Is the Markov chain irreducible?\n",
    "3. [3p] Is the Markov chain aperiodic? What is the period for each state?\n",
    "4. [3p] Does the Markov chain have a stationary distribution, and if so, what is it?\n",
    "5. [3p] Is the Markov chain reversible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 1\n",
    "\n",
    "#------------------------TRANSITION MATRIX -------------------------------\n",
    "# Answer each one by supplying the transition matrix as a numpy array\n",
    "# of shape (n_states,n_states), where state (A,B,...) corresponds to index (0,1,...)\n",
    "import numpy as np\n",
    "\n",
    "problem3_A = np.array([[0.8, 0.2, 0.0, 0.0],\n",
    "              [0.6, 0.2, 0.2, 0.0],\n",
    "              [0.0, 0.4, 0.0, 0.6],\n",
    "              [0.0, 0.0, 0.8, 0.2]])\n",
    "problem3_B   = np.array([[0.0,0.2,0.0,0.8],\n",
    "                          [0.0,0.0,1.0,0.0],\n",
    "                          [0.0,1.0,0.0,0.0],\n",
    "                          [0.5,0.0,0.5,0.0]])\n",
    "problem3_C   = np.array([[0.2,0.3,0.0,0.0,0.5],\n",
    "                          [0.2,0.2,0.6,0.0,0.0],\n",
    "                          [0.0,0.4,0.0,0.6,0.0],\n",
    "                          [0.0,0.0,0.0,0.6,0.4],\n",
    "                          [0.0,0.0,0.0,0.4,0.6]])\n",
    "problem3_D   = np.array([[0.8,0.2,0.0,0.0],\n",
    "                          [0.6,0.2,0.2,0.0],\n",
    "                          [0.0,0.4,0.0,0.6],\n",
    "                          [0.1,0.0,0.7,0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 2\n",
    "#------------------------REDUCIBLE -------------------------------\n",
    "# Answer each one with a True or False\n",
    "'''Irreducibility: a Markov chain is irreducible if its state space has only one connected class, \n",
    "i.e., the full membership of the state space,\n",
    "and the irreducibility of a Markov chain implies \n",
    "that the random variable can be transferred between any states during its evolution .'''\n",
    "problem3_A_irreducible = True\n",
    "problem3_B_irreducible = False\n",
    "problem3_C_irreducible = True\n",
    "problem3_D_irreducible = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 3\n",
    "#------------------------APERIODIC-------------------------------\n",
    "# Answer each one with a True or False\n",
    "'''Non-periodic: an MC,there does not exist a state from which the length of time elapsed \n",
    "before returning to this state is periodic, \n",
    "Theorem: Irreducible and non-periodic finite state Markov chains with a unique smooth distribution.'''\n",
    "problem3_A_is_aperiodic = True\n",
    "problem3_B_is_aperiodic = False\n",
    "problem3_C_is_aperiodic = False\n",
    "problem3_D_is_aperiodic = True\n",
    "\n",
    "# Answer the following with the period of the states as a numpy array\n",
    "# of shape (n_states,)\n",
    "\n",
    "problem3_A_periods = np.array([1,1,1,1])\n",
    "problem3_B_periods = np.array([1,1,1,1])\n",
    "problem3_C_periods = np.array([2,2,1,2,2])\n",
    "problem3_D_periods = np.array([2,2,2,2])\n",
    "#print(problem3_A_periods.shape)\n",
    "#print(problem3_B_periods.shape)\n",
    "#print(problem3_C_periods.shape)\n",
    "#print(problem3_D_periods.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 4\n",
    "import numpy as np\n",
    "def getstationary(P):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)  \n",
    "    stationary_distribution = np.real(eigenvectors[:, 0] / eigenvectors[:, 0].sum()) \n",
    "    answer_stationary = stationary_distribution\n",
    "    return answer_stationary\n",
    "def isver(P):\n",
    "    isnotinverse = 0\n",
    "    inverse =  1\n",
    "    import numpy as np\n",
    "    if np.linalg.det(P) == 0:\n",
    "        return isnotinverse\n",
    "    else:\n",
    "        return inverse\n",
    "#------------------------STATIONARY DISTRIBUTION-----------------\n",
    "# Answer each one with a True or False\n",
    "#A = isver(problem3_D)\n",
    "problem3_A_has_stationary = True\n",
    "problem3_B_has_stationary = True\n",
    "problem3_C_has_stationary = True\n",
    "problem3_D_has_stationary = True\n",
    "\n",
    "# Answer the following with the stationary distribution as a numpy array of shape (n_states,)\n",
    "# if the Markov chain has a stationary distribution otherwise answer with False\n",
    "#A = \n",
    "problem3_A_stationary_dist = getstationary(problem3_A)\n",
    "problem3_B_stationary_dist = getstationary(problem3_B)\n",
    "problem3_C_stationary_dist = getstationary(problem3_C)\n",
    "problem3_D_stationary_dist = getstationary(problem3_D)\n",
    "\n",
    "#print(problem3_A_stationary_dist.shape)\n",
    "#print(problem3_B_stationary_dist.shape)\n",
    "#print(problem3_C_stationary_dist.shape)\n",
    "#print(problem3_D_stationary_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 5\n",
    "#------------------------REVERSIBLE-----------------\n",
    "# Answer each one with a True or False\n",
    "'''The reversibility of a Markov chain is a stricter form of irreducibility,\n",
    "i.e., it not only transfers between arbitrary states, but also transfers to each state with equal probability, \n",
    "so that a reversible Markov chain is a sufficient non-necessary condition for a smooth Markov chain.'''\n",
    "problem3_A_is_reversible = False\n",
    "problem3_B_is_reversible = False\n",
    "problem3_C_is_reversible = False\n",
    "problem3_D_is_reversible = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "lx_assignment_number": "vB",
  "lx_course_instance": "2023",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
